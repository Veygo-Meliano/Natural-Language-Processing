# -*- coding: utf-8 -*-
"""submission_nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZFZ5NhjDm5nMFVUtHhN6F8Lz_gp59_8G
"""

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/dataset/sentiment_analysis_financial_news.csv', encoding='Windows-1252')
df.head()

#drop tabel yang tidak diperlukan
df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3'])
df.head()

#mengimpor beberapa library untuk text processing
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import string
import re
from collections import Counter

#download stopword untuk text processing
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')

#function untuk melakukan preprocessing
def remove_punct(text):
  text = "".join([char for char in text if char not in string.punctuation])
  text = re.sub('[0-9]+', '', text)
  return text

def remove_stopwords(text, STOPWORDS):
  #funtion untuk melakukan filltering
  return " ".join([word for word in str(text).split() if word not in STOPWORDS])

def stem_words(text, stemmer):
  #function untuk mengubah ke kata dasar (stemming)
  return " ".join([stemmer.stem(word) for word in text.split()])

def remove_freqwords(text, FREQWORDS):
  #function untuk menghapus kata yang sering keluar dalam 1 kalimat(hanya tampil 1)
  return " ".join([word for word in str(text).split() if word not in FREQWORDS])

def clean_review(text):
    clean_text = []
    for w in word_tokenize(text):
        if w.lower() not in stop:
            pos = pos_tag([w])
            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))
            clean_text.append(new_w)
    return clean_text

#melakukan filltering
df['sentence'] = df['sentence'].apply(lambda x: remove_punct(x))
", ".join(stopwords)
STOPWORDS = set(stopwords)
df["filltering"] = df["sentence"].apply(lambda text: remove_stopwords(text, STOPWORDS))
df.head()

#melakukan stemming
stemmer = PorterStemmer()
df["text_stemmed"] = df["filltering"].apply(lambda text: stem_words(text, stemmer))
df.head(3)

#melihat rata-rata kata yang sering keluar
cnt = Counter()
for text in df["text_stemmed"].values:
  for word in text.split():
      cnt[word] += 1
cnt.most_common(10)#melihat 10 kata yang sering keluar

#menghapus rata-rata kata yang sering keluar
FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])
df["text_stop_freq"] = df["text_stemmed"].apply(lambda text: remove_freqwords(text, FREQWORDS))
df.head(3)

#menghapus kolom hasil text processing kecuali hasil terakhir
df = df.drop(columns=["sentence", "filltering", "text_stemmed"])
df.head()

df = df.rename(columns={"text_stop_freq":"comments"})
df.head()

#merubah ke one-hot-encoding
category = pd.get_dummies(df.response)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='response')
df_baru

#mengubah nilai dataframe ke dalam tipe data array menggunakan atribut values
sentence = df_baru['comments'].values
label = df_baru[['negative', 'neutral', 'positive']].values

#membagi data menjadi data training dan data testing
from sklearn.model_selection import train_test_split
sentence_latih, sentence_test, label_latih, label_test = train_test_split(sentence, label, test_size=0.2)

#ubah setiap data ke numerik dengan Tokenizer
#setelah itu mengonversi sampel menjadi sequence

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=8766, oov_token='x')
tokenizer.fit_on_texts(sentence_latih)
tokenizer.fit_on_texts(sentence_test)

sekuens_latih = tokenizer.texts_to_sequences(sentence_latih)
sekuens_test = tokenizer.texts_to_sequences(sentence_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

from tensorflow.python.keras.layers.core import Dropout
import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=8766, output_dim=16),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(0.001),
              metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.75):
      print('\n akurasi sudah terpenuhi')
      self.model.stop_training = True
callback = myCallback()

history = model.fit(padded_latih, label_latih,
                    epochs=50,
                    batch_size=32,
                    
                    validation_data=(padded_test, label_test),
                    callbacks=[callback],
                    verbose=2)

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

